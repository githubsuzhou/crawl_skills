# -*- coding: utf-8 -*-
import scrapy
import re
import json,time,random
import xlsxwriter
from bs4 import BeautifulSoup
from collections import Counter

class SkillSpider(scrapy.Spider):
    name = 'skill'
    allowed_domains = ['www.lagou.com']
    url='https://www.lagou.com/jobs/'
    page=0
    data=[]
    start_urls = ['https://www.lagou.com/jobs/'+str(4925963)+'.html']
    position=[4925963, 5256792, 3183990, 3859972, 3476321, 4993753, 3056118, 5404728, 5269536, 4925963, 4231317, 4621034, 4780155, 5208361, 3062320, 2234156, 5401402, 4986957, 4136033, 3169424, 1306666, 5253284, 3606688, 5423715, 4682029, 4777867, 5224133, 5425622, 2219459, 3882833, 4476886, 5224172, 5267013, 4231189, 4737860, 4228159, 2553765, 4991494, 5423958, 4016680, 3154575, 1034397, 4508181, 5086892, 4003398, 4539164, 4914956, 3717807, 4202761, 5424377, 5426304, 5299651, 4229281, 5086130, 5118423, 4354458, 5204886, 5200874, 4794079, 4819295,
              4588357, 5286657, 236927, 5163256, 4204527, 3774911, 5269317, 5338767, 1546980, 5252026, 4473780, 3831036, 4190348, 1668885, 5071832, 3524356, 4808484, 4036299, 4886359, 4739952, 5315432, 4592977, 4978148, 2738872, 4747067, 4155824, 5182752, 5250882, 4866239, 5252101, 5313751, 4726142, 5073678, 5258790, 4753907, 5337999, 5280083, 5187786, 2082545, 3998586, 4684574, 5031600, 5074801, 3616330, 5094130, 5218214, 3207956, 5187904, 4912956, 5308675, 5201558, 5333792, 4400679, 3214480, 5154614, 5292961, 5260338, 5207655, 5207189, 4391021,
              5292368, 3571239, 5340166, 5316747, 3754446, 3388832, 5223650, 5114628, 4791530, 5341253, 4448075, 5266903, 5176588, 4993185, 3930979, 3827042, 5043949, 5323919, 2038368, 1229273, 4822458, 4753603, 5010244, 5311277, 4480145, 3940963, 2579200, 4763280, 492295, 3873845, 5292360, 5323886, 3917839, 3783175, 4600368, 5325300, 4702661, 5340018, 4567181, 5278357, 4135002, 4961946, 5184689, 4747978, 4505888, 5323837, 5339946, 2818570, 5054046, 2281397, 5247891, 5218312, 5422375, 4772109, 5359077, 4119424, 4938268, 5245180, 5078684, 5343341,
              5281004, 5323794, 5343933, 2813329, 5329806, 4898224, 4155421, 5344296, 4695914, 3308276, 4815556, 5198351, 4088075, 803668, 4687492, 3475304, 4721757, 3917864, 4081960, 4793099, 5277410, 5331113, 5329918, 4859915, 5328101, 5083371, 4138255, 5316303, 2040182, 4756386, 4860214, 5340168, 4324940, 4231536, 5293762, 5200269, 4762355, 2624027, 4997344, 5193457, 4335873, 5296695, 5362987, 5416220, 5351294, 5321545, 5159635, 5394505, 3810937, 5393184, 4961212, 5365116, 466441, 4820721, 4818117, 4841638, 4433337, 5076179, 5421915, 757538,
              4890809, 5351125, 5344170, 5415268, 5422916, 5378379, 3478400, 5400747, 5382498, 5407043, 5409184, 4812117, 5093210, 5249106, 5239474, 5384682, 5049684, 5412592, 5370781, 5418686, 5395075, 5161118, 5410728, 5378800, 5377564, 3776792, 4275756, 4522687, 5211654, 5113567, 3774208, 4692447, 4531352, 977539, 5323759, 3404774, 3141436, 5390843, 5407582, 3346396, 5347374, 5317060, 5401591, 5392093, 5197076, 5416031, 5392152, 5349530, 3716376, 5361158, 5367312, 3750009, 4681138, 5405579, 5341979, 2249592, 5413436, 5421288, 3832183, 5392038,
              5364189, 5360459, 5411651, 5346793, 5386389, 5408554, 5391002, 5305771, 3825944, 5370854, 5354151, 5414827, 5382888, 5291770, 4715645, 5339913, 5413385, 5391337, 4625288, 5187016, 4845309, 4523320, 4909788, 5418605, 5340058, 5383891, 5416952, 5363800, 5401454, 4320400, 5389376, 5165417, 4480733, 5365286, 1931439, 4464822, 4127630, 3886639, 5197490, 5423460, 5054396, 4752182, 5418573, 3048166, 5416795, 4055821, 5399477, 5362011, 5398975, 4830447, 4905745, 5085862, 5373458, 4658101, 5262751, 3001337, 5287394, 4335927, 4910986, 5326160,
              5331098, 4648293, 2997937, 4894901, 3182412, 4669608, 4055336, 4164906, 5333027, 3741676, 4532522, 5200914, 5332429, 4925676, 2748573, 5063400, 4644729, 5419552, 2276227, 5367342, 5384996, 5346252, 1791973, 1460139, 5386664, 4748028, 5357684, 2500528, 5407213, 5404262, 5353950, 5387546, 4699910, 2892813, 5217466, 4908592, 3144548, 3044542, 5363710, 3810370, 5422938, 3630761, 5421695, 5420754, 5370870, 5084542, 5391075, 3619742, 5409533, 5379714, 4914412, 5343373, 3792774, 4335307, 4191003, 5368043, 3180550, 5392849, 1766101, 5291428,
              5391128, 3622881, 2911959, 5066801, 4744220, 5034299, 5236833, 5151132, 3359183, 4908493, 5330877, 5010196, 5424563, 4362856, 5324706, 5387534, 4445543, 4216710, 5246596, 5420861, 4702292, 5401863, 5237511, 3359128, 5173044, 985961, 3910321, 5054205, 5383913, 4756088, 4566093, 4713422, 4479926, 4015195, 3837110, 4851721, 4238187, 3595544, 5194714, 5149457, 5365563, 4480356, 4908562, 5031056, 4993801, 657462, 5239489, 2418027, 5188486, 5407177, 3359099, 5000682, 5017523, 5243896, 3224371, 4756178, 3960849, 4613948, 2485079, 4448766
              ]
    # positions=[4925963, 5256792]  #测试的
    detail_header={
                    'Host':'www.lagou.com',
                    'Referer':'https://www.lagou.com/jobs/list_Java?city=%E5%85%A8%E5%9B%BD&cl=false&fromSearch=true&labelWords=&suginput=',
                    'Upgrade-Insecure-Requests': '1',
                    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'
                }
    def start_requests(self):
        print("请求第1页码")
        yield scrapy.Request(url=self.start_urls[0], callback=self.parse, headers=self.detail_header)

    def parse(self, response):
        # print(response.text)
        # print(response)
        data=response.text
        soup=BeautifulSoup(data,'lxml')
        job_detail=soup.find('dd',attrs={'class':'job_bt'})  #find_all没有。text
        text=job_detail.text  #去除标签
        print(text)
        rule = re.compile(r'[a-zA-Z]+')
        skill_list = rule.findall(text) #匹配到的英文
        for i in range(len(skill_list)):
            skill_list[i] = skill_list[i].lower()    #转换为小写
        self.data.extend(skill_list)
        print(self.data)
        if self.page<len(self.position)-1:
            time.sleep(random.randint(1,5))
            self.page+=1
            print("请求第%s页码"%(self.page+1))
            yield scrapy.Request(url=self.url+str(self.position[self.page])+'.html', callback=self.parse,headers=self.detail_header)
        else:
            if self.page==len(self.position)-1:
                count_dict = Counter(self.data).most_common(80) #统计其中80个的字符
                print(count_dict)
                # fname=input("请输入文件名称：")
                save_excel(count_dict=count_dict)




#
#
# #获取文本中的英文技能文本
# def search_skill(result):
#     rule = re.compile(r'[a-zA-Z]+')
#     skill_list = rule.findall(result)
#     return skill_list
#
# #统计技能
# def count_skill(skill_list):
#     for i in range(len(skill_list)):
#         skill_list[i] = skill_list[i].lower()
#     count_dict = Counter(skill_list).most_common(80) #统计其中80个的字符
#     return count_dict

#保存到excel中
# def save_excel(count_dict, file_name):
#     book = xlsxwriter.Workbook(
#         r'D:\程序测试\Scrapy\lagoujineng\{0}.xls'.format(file_name))
def save_excel(count_dict):
    book = xlsxwriter.Workbook('D:\\程序测试\\Scrapy\\skills_lagou\\skills_lagou.xls')
    tmp = book.add_worksheet()
    row_num = len(count_dict)
    for i in range(1, row_num):
        if i == 1:
            tag_pos = 'A%s' % i
            tmp.write_row(tag_pos, ['关键词', '频次'])
        else:
            con_pos = 'A%s' % i
            k_v = list(count_dict[i - 2])
            tmp.write_row(con_pos, k_v)
    chart1 = book.add_chart({'type': 'area'})
    chart1.add_series({
        'name': '=Sheet1!$B$1',
        'categories': '=Sheet1!$A$2:$A$80',
        'values': '=Sheet1!$B$2:$B$80'
    })
    chart1.set_title({'name': '关键词排名'})
    chart1.set_x_axis({'name': '关键词'})
    chart1.set_y_axis({'name': '频次(/次)'})
    tmp.insert_chart('C2', chart1, {'x_offset': 15, 'y_offset': 10})
    book.close()

